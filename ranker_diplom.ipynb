{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_REh1ofoKjlk",
        "outputId": "33c9cbac-cac6-450b-8a1c-076c06b93d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWgUxaEhZm83"
      },
      "outputs": [],
      "source": [
        "# Загрузка токенизатора и модели\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\", torch_dtype=torch.float16).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xKs9IHO-zjE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = load_dataset(\"ashraq/movielens_ratings\", split=\"train\")\n",
        "\n",
        "n = 10000\n",
        "small_dataset = dataset.select(range(n))\n",
        "train_dataset = small_dataset.select(range(int(0.8 * n)))\n",
        "eval_dataset = small_dataset.select(range(int(0.8 * n), n))\n",
        "\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"ashraq/movielens_ratings\", split=\"train\")\n",
        "\n",
        "n = 1000\n",
        "indices = list(range(n))\n",
        "train_indices = indices[:int(0.8 * n)]\n",
        "eval_indices = indices[int(0.8 * n):]\n",
        "\n",
        "small_dataset = dataset.select(indices)\n",
        "train_dataset = small_dataset.select(train_indices)\n",
        "eval_dataset = small_dataset.select(eval_indices)\n",
        "\n",
        "\n",
        "class MovieLensHFDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_length=128):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.pairs = [(i, j) for i in range(len(dataset)) for j in range(len(dataset))]\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        idx1, idx2 = self.pairs[idx]\n",
        "        row1 = self.dataset[idx1]\n",
        "        row2 = self.dataset[idx2]\n",
        "\n",
        "        user_id1 = row1[\"user_id\"]\n",
        "        movie_title1 = row1[\"title\"]\n",
        "        genres1 = row1[\"genres\"]\n",
        "        rating1 = row1[\"rating\"]\n",
        "\n",
        "        user_id2 = row2[\"user_id\"]\n",
        "        movie_title2 = row2[\"title\"]\n",
        "        genres2 = row2[\"genres\"]\n",
        "        rating2 = row2[\"rating\"]\n",
        "\n",
        "        prompt1 = (\n",
        "            f\"Movie 1\\n\"\n",
        "            f\"User ID: {user_id1}\\n\"\n",
        "            f\"Movie title: {movie_title1}\\n\"\n",
        "            f\"Genres: {genres1}\\n\"\n",
        "        )\n",
        "\n",
        "        prompt2 = (\n",
        "            \"Movie 2\\n\"\n",
        "            f\"User ID: {user_id2}\\n\"\n",
        "            f\"Movie title: {movie_title2}\\n\"\n",
        "            f\"Genres: {genres2}\\n\"\n",
        "        )\n",
        "\n",
        "        target = 1 if rating1 >= rating2 else 0\n",
        "\n",
        "        inputs1 = self.tokenizer(\n",
        "            prompt1,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        inputs2 = self.tokenizer(\n",
        "            prompt2,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(device)\n",
        "\n",
        "        x1 = {key: val.squeeze(0) for key, val in inputs1.items() if isinstance(key, (int, float, str))}\n",
        "        x2 = {key: val.squeeze(0) for key, val in inputs2.items() if isinstance(key, (int, float, str))}\n",
        "\n",
        "\n",
        "        return x1, x2, torch.tensor(target).to(device)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # assuming all possible pairs could be generated\n",
        "        return len(self.dataset) * len(self.dataset)\n",
        "\n",
        "\n",
        "train = MovieLensHFDataset(train_dataset, tokenizer)\n",
        "val = MovieLensHFDataset(eval_dataset, tokenizer)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soKrG-EvewSq"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"NLP project\",\n",
        "    reinit=True,\n",
        "    # track hyperparameters and run metadata,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZIBQSMyCLjf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RankingModel(nn.Module):\n",
        "    def __init__(self, hidden_size=768, num_classes=1):\n",
        "        super(RankingModel, self).__init__()\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense(x)\n",
        "\n",
        "\n",
        "rank_model = RankingModel().to(device)\n",
        "criterion = nn.MarginRankingLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "g4H1wIEmMfte",
        "outputId": "c63d9626-e032-464a-94b8-ae0e93979211"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "No active exception to reraise",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-9c9a2cba73bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ],
      "source": [
        "raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziwc_kHu-zoK",
        "outputId": "7ba9574d-7191-4cfa-c82d-4302c33da31e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [25:02<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 0: 0.004201131435763091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [01:33<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results - Accuracy: 0.504875, Precision: 0.5839187345226664, Recall:0.49961082763988585, F1: 0.538484841423345, ROC-AUC: 0.5040365670824895\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [24:51<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 1: 6.019460805691778e-06\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [01:33<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results - Accuracy: 0.5204, Precision: 0.5913515016685206, Recall:0.5517166825218369, F1: 0.5708469419712765, ROC-AUC: 0.5161560087862863\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2500/2500 [24:49<00:00,  1.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 2: 2.2266721352934837e-07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [01:33<00:00,  1.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Results - Accuracy: 0.46895, Precision: 0.548631905007744, Recall:0.45952607454812766, F1: 0.5001411897590361, ROC-AUC: 0.45289448435551505\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "from transformers import Trainer, BertModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class RankingTrainer:\n",
        "    def __init__(self, base_model, rank_model, criterion, optimizer_cls=Adam, lr=0.00001):\n",
        "        self.base_model = base_model\n",
        "        self.rank_model = rank_model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer_cls(self.rank_model.parameters(), lr=lr)\n",
        "\n",
        "    def train(self, train_dataset, validation_dataset, epochs, batch_size):\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "        self.rank_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for x1, x2, y_true in tqdm(train_dataloader):\n",
        "                self.optimizer.zero_grad()\n",
        "                with torch.no_grad():\n",
        "                    output1, output2 = self.base_model(**x1), self.base_model(**x2)\n",
        "                y1, y2 = output1.last_hidden_state[:, 0, :].to(torch.float32), output2.last_hidden_state[:, 0, :].to(torch.float32)\n",
        "                y1, y2 = self.rank_model(y1)[:, 0], self.rank_model(y2)[:, 0]\n",
        "                loss = self.criterion(y1, y2, y_true)\n",
        "                loss.backward()\n",
        "                total_loss += loss.item()\n",
        "                self.optimizer.step()\n",
        "            avg_loss = total_loss / len(train_dataloader)\n",
        "            print(f'Training loss at epoch {epoch}: {avg_loss}')\n",
        "            wandb.log({\n",
        "                \"epoch\": \"epoch\",\n",
        "                \"train_loss\": avg_loss,\n",
        "            })\n",
        "            self.evaluate(validation_dataloader)\n",
        "\n",
        "\n",
        "    def evaluate(self, dataloader):\n",
        "        self.rank_model.eval()\n",
        "        total_loss = 0\n",
        "        pred_scores = []\n",
        "        true_scores = []\n",
        "        with torch.no_grad():\n",
        "            for x1, x2, y_true in tqdm(dataloader):\n",
        "                output1, output2 = self.base_model(**x1), self.base_model(**x2)\n",
        "                y1, y2 = output1.last_hidden_state[:, 0, :].to(torch.float32), output2.last_hidden_state[:, 0, :].to(torch.float32)\n",
        "                y1, y2 = self.rank_model(y1)[:, 0], self.rank_model(y2)[:, 0]\n",
        "                pred_scores.extend((y1-y2).cpu().numpy())\n",
        "                true_scores.extend(y_true.cpu().numpy())\n",
        "\n",
        "        pred_labels = [1 if score > 0 else -1 for score in pred_scores]\n",
        "\n",
        "        accuracy = accuracy_score(true_scores, pred_labels)\n",
        "        precision = precision_score(true_scores, pred_labels)\n",
        "        recall = recall_score(true_scores, pred_labels)\n",
        "        f1 = f1_score(true_scores, pred_labels)\n",
        "        roc_auc = roc_auc_score(true_scores, pred_scores)\n",
        "        print(f'Evaluation Results - Accuracy: {accuracy}, Precision: {precision}, Recall:{recall}, F1: {f1}, ROC-AUC: {roc_auc}')\n",
        "\n",
        "        wandb.log({\n",
        "            \"val_accuracy\": accuracy,\n",
        "            \"val_precision\": precision,\n",
        "            \"val_recall\": recall,\n",
        "            \"val_f1\": f1,\n",
        "            \"val_roc_auc\": roc_auc\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rank_trainer = RankingTrainer(model, rank_model, criterion)\n",
        "rank_trainer.train(train, val, epochs=3, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npJjf_ce-zvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9152a52-d9d0-498e-d40a-fae8c0a94e9d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 2500/2500 [25:23<00:00,  1.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] train loss: 0.6888\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:35<00:00,  1.64it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val: Loss=0.6734 Acc=0.6175 Prec=0.6458 Rec=0.7494 F1=0.6938 ROC_AUC=0.6412\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 2500/2500 [25:29<00:00,  1.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] train loss: 0.6511\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:34<00:00,  1.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val: Loss=0.6365 Acc=0.6451 Prec=0.6576 Rec=0.8055 F1=0.7241 ROC_AUC=0.6778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 2500/2500 [25:23<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] train loss: 0.6000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 157/157 [01:34<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val: Loss=0.6564 Acc=0.6258 Prec=0.6505 Rec=0.7622 F1=0.7020 ROC_AUC=0.6472\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import wandb  # Убедитесь, что wandb импортирован, если вы его используете\n",
        "\n",
        "class CEPairwiseTrainer:\n",
        "    def __init__(self, encoder, ranker, lr=1e-5, device=\"cpu\"):\n",
        "        self.encoder = encoder.to(device)\n",
        "        self.ranker = ranker.to(device)\n",
        "        self.criterion = BCEWithLogitsLoss()\n",
        "        self.optimizer = Adam(self.ranker.parameters(), lr=lr)\n",
        "        self.device = device\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, epochs=1, batch_size=32):\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        self.encoder.eval()\n",
        "        self.ranker.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            for x1, x2, y_true in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "                self.optimizer.zero_grad()\n",
        "                x1 = {k: v.to(self.device) for k, v in x1.items()}\n",
        "                x2 = {k: v.to(self.device) for k, v in x2.items()}\n",
        "                y_true = y_true.to(self.device).float()  # (batch,) 0.0 or 1.0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    h1 = self.encoder(**x1).last_hidden_state[:, 0, :]  # [CLS]\n",
        "                    h2 = self.encoder(**x2).last_hidden_state[:, 0, :]\n",
        "                target_dtype = next(self.ranker.parameters()).dtype\n",
        "                target_device = next(self.ranker.parameters()).device\n",
        "                h1 = h1.to(dtype=target_dtype, device=target_device)\n",
        "                h2 = h2.to(dtype=target_dtype, device=target_device)\n",
        "\n",
        "                s1 = self.ranker(h1)[:, 0]\n",
        "                s2 = self.ranker(h2)[:, 0]\n",
        "\n",
        "                logits = s1 - s2\n",
        "                loss = self.criterion(logits, y_true)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            print(f'[Epoch {epoch+1}] train loss: {avg_loss:.4f}')\n",
        "            self.evaluate(val_loader)\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        self.encoder.eval()\n",
        "        self.ranker.eval()\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "        all_preds = []  # Добавляем список для накопления всех предсказаний\n",
        "        total_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for x1, x2, y_true in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "                x1 = {k: v.to(self.device) for k, v in x1.items()}\n",
        "                x2 = {k: v.to(self.device) for k, v in x2.items()}\n",
        "                y_true = y_true.to(self.device).float()  # (batch,)\n",
        "\n",
        "                h1 = self.encoder(**x1).last_hidden_state[:, 0, :]\n",
        "                h2 = self.encoder(**x2).last_hidden_state[:, 0, :]\n",
        "\n",
        "                target_dtype = next(self.ranker.parameters()).dtype\n",
        "                target_device = next(self.ranker.parameters()).device\n",
        "                h1 = h1.to(dtype=target_dtype, device=target_device)\n",
        "                h2 = h2.to(dtype=target_dtype, device=target_device)\n",
        "\n",
        "                s1 = self.ranker(h1)[:, 0]\n",
        "                s2 = self.ranker(h2)[:, 0]\n",
        "                logits = s1 - s2\n",
        "\n",
        "                loss = self.criterion(logits, y_true)\n",
        "                total_loss += loss.item()\n",
        "                probs = torch.sigmoid(logits)\n",
        "                batch_preds = (probs > 0.5).long().cpu().numpy()\n",
        "\n",
        "                all_scores.extend(probs.cpu().numpy())\n",
        "                all_labels.extend(y_true.cpu().numpy())\n",
        "                all_preds.extend(batch_preds)  # Накапливаем предсказания из всех батчей\n",
        "\n",
        "        # Используем накопленные предсказания вместо preds из последнего батча\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        precision = precision_score(all_labels, all_preds, zero_division=0, average='binary', pos_label=1)\n",
        "        recall = recall_score(all_labels, all_preds, zero_division=0, average='binary', pos_label=1)\n",
        "        f1 = f1_score(all_labels, all_preds, zero_division=0, average='binary', pos_label=1)\n",
        "        try:\n",
        "            roc_auc = roc_auc_score(all_labels, all_scores)\n",
        "        except:\n",
        "            roc_auc = float('nan')\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f'Val: Loss={avg_loss:.4f} Acc={accuracy:.4f} Prec={precision:.4f} Rec={recall:.4f} F1={f1:.4f} ROC_AUC={roc_auc:.4f}')\n",
        "\n",
        "        # Убедитесь, что wandb импортирован, если вы используете этот код\n",
        "        wandb.log({\n",
        "            \"val_accuracy\": accuracy,\n",
        "            \"val_precision\": precision,\n",
        "            \"val_recall\": recall,\n",
        "            \"val_f1\": f1,\n",
        "            \"val_roc_auc\": roc_auc\n",
        "        })\n",
        "\n",
        "# Пример инициализации:\n",
        "trainer = CEPairwiseTrainer(model, rank_model, lr=1e-5, device=\"cuda\")\n",
        "trainer.train(train, val, epochs=3, batch_size=256)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OccOi9-a-zx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257ea082-991c-4474-ea7a-66008ef31d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/40000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for x1, x2, y_true in tqdm(val):\n",
        "    print(y_true)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD2uGJSO-zz3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}